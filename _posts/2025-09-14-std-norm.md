---
layout: post
title: "Normalization vs Standardization - What to pick?"
date: 2025-09-14 10:00:00 -0400
math: true
image:
  path: https://tushar-bioinfo.github.io/learning-bioinformatics/assets/img/blog5/cover.png
---

## TL;DR (Cheat-Sheet)
- **Normalization (Minâ€“Max):** squeezes values into a fixed range, usually [0, 1].  
- **Standardization (Z-score):** shifts mean to 0 and scales spread to 1.  
- **Why scale?** Puts features on a fair scale so distance- or gradient-based models donâ€™t get biased by â€œbig numberâ€ features.  
- **Use scaling for:** kNN, SVM, Logistic/Linear Regression, Neural Nets, PCA, k-Means.  
- **Skip scaling for:** Tree-based models (Decision Trees, Random Forests, XGBoost) â€” they donâ€™t care about scale.  

---

## Why Scaling Matters
Imagine two features:
- `Glucose`: values around **3000**  
- `GeneScore`: values around **0.01**

To a distance-based model, `Glucose` completely dominates. Scaling fixes this imbalance, letting models treat both features fairly.  
{: .prompt-tip }

---

## Before Scaling

![Plot1](https://tushar-bioinfo.github.io/learning-bioinformatics/assets/img/blog5/plot1.png){: width="700" height="700" }

> **Observation:** x-axis spans thousands, y-axis hugs near zero. Without scaling, the smaller-scale feature is almost invisible to the model. 
---

## After Minâ€“Max Normalization
**Formula:**  


## After Minâ€“Max Normalization
**Formula:**  

$$
x_{norm} = \frac{x - x_{min}}{x_{max} - x_{min}}
$$

![Bar Plot](https://tushar-bioinfo.github.io/learning-bioinformatics/assets/img/blog5/plot2.png){: width="700" height="700" }



> **How to read this:** Both features now live between 0 and 1. The shapes are preserved, but the scale is uniform. A step of 0.1 means the same normalized change for both features.  
{: .prompt-warning }

---

## After Standardization (Z-score)
**Formula:**  

$$
z = \frac{x - \mu}{\sigma}
$$

![Bar Plot](https://tushar-bioinfo.github.io/learning-bioinformatics/assets/img/blog5/plot2.png){: width="700" height="700" }



> **How to read this:** Both features now live between 0 and 1. The shapes are preserved, but the scale is uniform. A step of 0.1 means the same normalized change for both features.  
{: .prompt-warning }

---

## After Standardization (Z-score)
**Formula:**  

$$
x_{norm} = \frac{x - x_{min}}{x_{max} - x_{min}}
$$

![Bar Plot](https://tushar-bioinfo.github.io/learning-bioinformatics/assets/img/blog5/plot3.png){: width="700" height="700" }



> **How to read this:** Features are centered around 0 with similar spread (~1). Models like Logistic Regression, SVM, and many Neural Nets train more stably here.  
{: .prompt-tip }

---

## When to Use What
- **Normalization (Minâ€“Max):** when you want strict bounds [0, 1] or when the next step (like a neural net with sigmoid activations) expects inputs in that range.  
- **Standardization (Z-score):** strong general default, especially for gradient-based models and PCA.  
- **No scaling needed:** tree-based models (they split on thresholds, not distances).  

---

## Common Pitfalls
1. **Data leakage:** Always fit the scaler on **training only**, not the whole dataset.  
2. **Scaling categories:** Donâ€™t scale encoded categorical values (they arenâ€™t â€œmagnitudesâ€).  
3. **Outliers:** Minâ€“Max and Z-score can get distorted. (Outlier-robust scaling is a later topic.)  

---

## Final Notes
Scaling is your **fairness tool** for features:  
- Normalization = squeeze into bounds.  
- Standardization = recenter + rescale.  
- Use them whenever magnitudes differ wildly **and** your model cares about distances or gradients.  

**Full notebook:** [GitHub](https://github.com/Tushar-bioinfo/Blogs/tree/main/blog5)  

Happy scaling! ğŸš€
